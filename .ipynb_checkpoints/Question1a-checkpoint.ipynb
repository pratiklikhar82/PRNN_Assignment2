{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as seabornInstance \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.loadtxt(\"Gamma_train.txt\",dtype='float',delimiter=',',skiprows=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.9646  0.5897  1.    ]\n",
      " [ 1.5701  0.0872  1.    ]\n",
      " [ 3.3763  0.1963  1.    ]\n",
      " ...\n",
      " [ 7.2153  3.5391 -1.    ]\n",
      " [ 5.2954  5.938  -1.    ]\n",
      " [ 3.4157  6.1094 -1.    ]]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.9646 0.5897 1.    ]\n"
     ]
    }
   ],
   "source": [
    "print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.9646 0.5897]\n",
      " [1.5701 0.0872]\n",
      " [3.3763 0.1963]\n",
      " ...\n",
      " [7.2153 3.5391]\n",
      " [5.2954 5.938 ]\n",
      " [3.4157 6.1094]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1. ... -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for training set size 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "index10 = np.random.randint(0,2000,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 798  248 1390 1970 1561  832 1535  853  993 1652]\n"
     ]
    }
   ],
   "source": [
    "print(index10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_10 = X_train[index10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.4352 0.4933]\n",
      " [0.2232 0.2455]\n",
      " [1.181  1.4787]\n",
      " [1.0615 4.5001]\n",
      " [4.1867 9.2189]\n",
      " [1.0698 0.5289]\n",
      " [1.8276 6.8188]\n",
      " [0.0502 0.0302]\n",
      " [0.01   0.6312]\n",
      " [5.7129 6.7265]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_10 = Y_train[index10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1. -1. -1. -1.  1. -1.  1.  1. -1.]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_10 = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor_10.fit(X_train_10, Y_train_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.loadtxt(\"Gamma_test.txt\",dtype='float',delimiter=',',skiprows=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0509 0.0399]\n",
      " [0.8071 0.2009]\n",
      " [1.1025 0.164 ]\n",
      " ...\n",
      " [3.5262 0.1474]\n",
      " [2.355  4.6955]\n",
      " [4.4955 9.8387]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_10 = regressor_10.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.37817895e-01  7.36029026e-01  7.62856464e-01  7.46503657e-01\n",
      "  6.56883164e-01  7.65572645e-01  7.19658044e-01  4.54358324e-01\n",
      "  7.94959538e-01  7.09557818e-01  7.71986285e-01  7.44748986e-01\n",
      "  7.40828519e-01  4.45606337e-01  7.67364159e-01  5.71717350e-01\n",
      "  7.99196052e-01  7.38343975e-01  1.88415496e-01  7.56312026e-01\n",
      "  7.42710707e-01  8.82696759e-01  7.27498199e-01  7.03846818e-01\n",
      "  6.41355266e-01  7.37467991e-01  4.89806539e-01  7.18514504e-01\n",
      "  5.46791431e-01  5.62129380e-01  4.94315553e-01  7.49501815e-01\n",
      "  7.54883445e-01  7.37576185e-01 -1.25768387e+00  7.15206265e-01\n",
      "  5.68736080e-01  6.43706897e-01  7.79801745e-01  4.83722544e-01\n",
      "  7.59359418e-01  6.50633402e-01  5.32934581e-01  7.42422097e-01\n",
      "  6.57364288e-01  7.71363996e-01 -5.11756180e-01  7.19783725e-01\n",
      "  7.05076649e-01  8.02255871e-01  7.49166103e-01  6.80741151e-01\n",
      "  7.45746095e-01  7.48395937e-01  6.68573497e-01  7.59441316e-01\n",
      "  4.65875056e-01  8.30145832e-01 -3.13007487e-01  7.48152976e-01\n",
      "  4.06829775e-01  2.76486783e-01  6.33434099e-01  2.93969137e-01\n",
      "  7.55172239e-01  5.64084528e-01  7.51735836e-01  7.45991262e-01\n",
      "  1.34148274e-02  7.49550157e-01  1.76362866e-01  1.88935400e-01\n",
      "  7.71677230e-01  7.41948951e-01  7.04143643e-01  7.24528303e-01\n",
      "  5.89725167e-01  3.85568751e-01  6.66203520e-01  7.92229621e-01\n",
      "  7.38683083e-01  7.35150222e-01  7.42140319e-01  7.76418306e-01\n",
      "  5.00216108e-01  7.52219366e-01  6.90316142e-01  7.49011030e-01\n",
      "  5.75433892e-01  7.20299398e-01  7.65156400e-01  6.43531546e-01\n",
      "  5.64652261e-01  7.53824136e-01  7.78975454e-01  6.68456704e-01\n",
      "  1.26793059e-01  6.50688528e-01  7.44185448e-01  6.98382230e-01\n",
      "  5.94080840e-01  7.72822719e-01  7.58054439e-01  5.75376673e-01\n",
      "  6.91004335e-01  6.30940731e-01  8.02614707e-01  6.79161786e-01\n",
      "  7.56936332e-01  5.22109563e-01  7.45971801e-01  3.74246527e-01\n",
      "  6.87621372e-01  7.21156154e-01  3.81106330e-01  6.34214733e-01\n",
      "  7.48362718e-01  6.52608714e-01  7.78387289e-01  7.42200145e-01\n",
      "  7.13043376e-01  6.36923772e-01  7.65390475e-01  6.78948419e-01\n",
      "  4.50510443e-01  7.50505314e-01  7.02158029e-01  6.07041718e-01\n",
      "  8.23482375e-01  6.50601372e-01  7.39804477e-01  7.61180716e-01\n",
      "  5.91494209e-01  7.44594572e-01  7.59464788e-01  5.73399624e-01\n",
      "  7.86956400e-03  6.59090218e-01  7.24092928e-01  6.57640242e-01\n",
      "  6.85958857e-01  8.22823312e-01  4.04207001e-01  1.26477873e-01\n",
      "  8.09083285e-01  7.38724586e-01  3.44758427e-01  6.71843634e-01\n",
      "  8.37323503e-01  6.71467236e-01  5.70701576e-01  7.10548509e-01\n",
      "  5.87279840e-01  7.40712601e-01  7.31919742e-01  5.88528193e-01\n",
      "  7.70520118e-01  8.11671020e-01  7.44641041e-01  8.62631155e-01\n",
      "  7.06886151e-01  7.43585866e-01  6.81555741e-01  7.43795993e-01\n",
      "  7.23838409e-01  7.60515076e-01  2.70046318e-02  8.08627166e-01\n",
      "  6.70797452e-01  7.53858185e-01  6.90823498e-01  7.55076216e-01\n",
      "  2.41692620e-01  7.82979037e-01  4.61139884e-01  7.52916456e-01\n",
      "  7.22151152e-01  7.66279273e-01  3.42964249e-01  7.43579030e-01\n",
      "  3.02309379e-01  7.18509859e-01  5.43831257e-01  6.80943745e-01\n",
      "  7.45450372e-01  7.92699151e-01  7.33813124e-01  6.79180452e-01\n",
      "  7.76893261e-01  6.91320928e-01  6.53179103e-01  4.03896491e-01\n",
      "  5.13720446e-01  7.55262544e-01  6.92631677e-01  7.37759411e-01\n",
      "  7.38738672e-01  7.31957403e-01  6.60005625e-01  6.13861507e-01\n",
      "  6.78796051e-01  8.20432598e-01  6.15389868e-01  7.38241353e-01\n",
      "  6.31207831e-01  6.29774048e-01  6.96414360e-01  6.25209770e-01\n",
      "  7.49916541e-01  3.75305907e-01  4.08481037e-01  7.40975952e-01\n",
      "  7.03844429e-01  7.07827848e-01  7.46005954e-01  6.49841711e-01\n",
      "  7.87360698e-01  2.47214247e-01  6.20258435e-01  6.58287989e-01\n",
      "  5.26699779e-01  5.75834554e-01  7.37017883e-01  5.69817576e-01\n",
      "  6.94615442e-01  7.02579209e-01  5.59580768e-01  7.87719638e-01\n",
      "  6.61321813e-01  4.02180974e-01 -3.99861131e-01  7.44685877e-01\n",
      "  7.67854165e-01  7.48499928e-01  6.98500704e-01  4.56953997e-01\n",
      "  7.31802817e-01  7.63986519e-01  1.41820172e-02  4.47569539e-01\n",
      "  7.48725136e-01  5.66543855e-01  7.48655014e-01  7.23142770e-01\n",
      "  7.68999828e-01  7.52482705e-01  7.87847795e-01  7.24103546e-01\n",
      "  6.27373031e-01  3.57062093e-01  7.74521717e-01  6.91689243e-01\n",
      "  5.29168019e-01  5.37507072e-01  7.57153944e-01  5.86290196e-01\n",
      "  6.87564286e-01  1.42481005e-01  2.61104239e-01  7.88752469e-01\n",
      "  7.58506906e-01  8.01214900e-01  7.85459064e-01  7.70934849e-01\n",
      "  2.89580083e-01  5.98507101e-01  7.51749590e-01  7.58650281e-01\n",
      "  7.58941162e-01  6.30146631e-01  6.42311600e-01  6.99401092e-01\n",
      "  5.77592806e-01  6.30640083e-03  6.91378541e-01  6.84752880e-01\n",
      "  6.43663710e-01 -1.53453116e-01  3.59474683e-01  5.06947990e-01\n",
      "  7.63198092e-01  7.81716344e-01  5.91095574e-01  7.50378217e-01\n",
      " -6.84193485e-01  3.23259241e-01  7.29802833e-01  7.44983738e-01\n",
      "  6.23303549e-01  4.89804347e-01  7.70318587e-01 -7.21350128e-01\n",
      "  8.41202389e-01  7.51129474e-01  5.12820682e-01  7.85051696e-01\n",
      "  7.45960529e-01  8.34509737e-01  6.59394608e-01  6.68088956e-01\n",
      "  1.69023220e-01  7.66642253e-01  4.91203628e-01  3.17640732e-01\n",
      "  7.56156023e-01  7.49733384e-01  6.96716038e-01  7.09655088e-01\n",
      "  7.78328005e-01  5.02091908e-01  7.08936539e-01  7.50240746e-01\n",
      "  6.33626014e-01  7.50771610e-01 -5.84078632e-01  7.51086132e-01\n",
      "  7.46855109e-01  4.04460099e-01  7.93986052e-01  6.10480497e-01\n",
      "  5.97306823e-01  7.28215258e-01  6.61432159e-01  6.94105000e-01\n",
      "  7.46872731e-01  7.06763432e-01  7.42274224e-01  7.90022886e-01\n",
      "  6.89590885e-01 -3.74691299e-01  1.88750816e-01  7.79329759e-01\n",
      "  7.06953744e-01  7.99407285e-01  7.38209682e-01  5.59145871e-01\n",
      "  7.13496302e-01  7.57600028e-01  7.10354277e-01  4.18367788e-01\n",
      "  1.69816919e-02  6.55635856e-01  7.45873934e-01  7.45286437e-01\n",
      "  7.32545556e-01  7.79481697e-01  7.61526726e-01  6.22905848e-01\n",
      "  7.54382495e-01  7.91677273e-01  7.65428350e-01  5.58869637e-01\n",
      "  6.79963144e-01  4.99868486e-01  5.99525066e-01  6.04196797e-01\n",
      "  7.32756684e-01  1.82949748e-01  7.73779573e-01  8.70616045e-01\n",
      "  7.14917518e-01  7.38485209e-01  7.27951950e-01  2.96096900e-01\n",
      "  8.62189548e-01  2.26307161e-01  7.08747798e-01  7.26132189e-01\n",
      "  5.90988725e-01  5.42122297e-01  6.89728309e-01  7.48994041e-01\n",
      "  4.45429897e-01  1.07982404e-01  7.52585161e-01  6.69139013e-01\n",
      "  7.94078700e-01  7.32449141e-01  4.39917579e-01  5.71286902e-01\n",
      "  7.60059720e-01  5.76119652e-01  7.16224394e-01  6.35859367e-01\n",
      "  7.13463507e-01  4.64739427e-01  7.47301930e-01  7.48263656e-01\n",
      "  3.47863529e-01  8.37569776e-01  7.80831738e-01  7.43713398e-01\n",
      "  5.34435414e-01  5.50761312e-01  6.52861386e-01  7.40968960e-01\n",
      "  7.42977338e-01  5.72074474e-01  6.95786852e-01  7.83934512e-01\n",
      "  5.65329656e-01  7.23058935e-01  5.03340863e-01  7.47633987e-01\n",
      "  7.42087987e-01  6.38186400e-01  7.46568467e-01  7.24839303e-01\n",
      "  7.47086898e-01  5.49347660e-01  7.64719598e-01  7.52019863e-01\n",
      "  7.50071401e-01  6.55826036e-01  4.47146349e-01  7.56760307e-01\n",
      "  6.90519554e-01  7.44543696e-01  5.17323486e-01  7.50004179e-01\n",
      "  7.52699824e-01  7.73017309e-01  7.52245425e-01  6.80202360e-01\n",
      "  6.77324352e-01  5.78419075e-01  7.16445131e-01  4.37186483e-01\n",
      "  7.38258755e-01  7.50175403e-01  7.66169215e-01  7.45956087e-01\n",
      "  8.02557932e-01  7.26282231e-01  7.24675932e-01  6.46228709e-01\n",
      "  7.82719124e-01  7.04577909e-01  4.19749795e-01  6.04335224e-01\n",
      "  6.81605343e-01  7.28365844e-01  4.49873223e-01  3.84776315e-01\n",
      "  7.00648965e-01  7.50910449e-01  7.14549140e-01  7.14923078e-01\n",
      "  7.03394779e-01  7.98974108e-01  7.80742946e-01  7.21333229e-01\n",
      "  7.75969264e-01  7.49091862e-01  7.39794642e-01  6.58591407e-01\n",
      "  7.31181379e-01  6.11274289e-01  7.36438839e-01  6.36607986e-01\n",
      "  5.67635422e-01  6.73353793e-01  7.52508909e-01  7.43437755e-01\n",
      "  7.61263835e-01  6.11309509e-01  5.88048331e-01  7.19992699e-01\n",
      "  6.74332655e-01  7.56292560e-01  7.35234230e-01  7.51145004e-01\n",
      "  5.19168793e-01  7.80210193e-01  5.68612085e-01  7.23172991e-01\n",
      "  7.38544840e-01  7.51036126e-01  8.25827622e-01  4.32882626e-01\n",
      "  7.18407400e-01  7.25780667e-01  6.99855037e-01  4.89436926e-01\n",
      "  7.12424855e-01  2.24692553e-01  4.68258383e-01  8.24506215e-01\n",
      "  7.92331071e-01  6.50867853e-01  7.39220256e-01  7.45713548e-01\n",
      "  6.48183905e-01  7.37231180e-01  6.44302548e-01  3.88097438e-01\n",
      "  7.13733938e-01  7.71218790e-01  4.97522191e-01  4.97188174e-01\n",
      "  3.65957038e-01 -4.50930123e+00 -2.22614389e-01  7.63409642e-01\n",
      " -3.09932592e-01 -8.45229448e-01 -5.40919883e-01 -1.11699522e+00\n",
      "  6.59855264e-01 -1.83700837e-01  4.87299113e-01  1.98867603e-01\n",
      "  2.92482311e-01 -8.25056579e-01 -2.08075159e-01  7.99288757e-01\n",
      " -4.33780475e-01  1.27042884e-01  6.28977874e-01  6.62951144e-02\n",
      "  5.50910074e-01  2.21987262e-01 -5.42770681e-01  5.88403707e-01\n",
      "  3.43448967e-01  1.52192470e-01 -4.73571399e-01  1.99965122e-01\n",
      "  4.85909594e-01 -6.45066339e-01  6.03038164e-01 -5.15226009e-01\n",
      " -4.71131910e-01  5.84198685e-01 -4.51913348e-01 -4.31212747e-02\n",
      " -2.41611098e+00  2.35759817e-01 -8.10185338e-01  1.13726097e-02\n",
      " -3.30432108e+00  6.56807894e-01 -8.66120405e-01 -8.00597877e-01\n",
      " -7.43972507e-01 -2.99450436e-01 -6.74686655e-02 -2.34723336e-01\n",
      "  3.53578766e-02 -9.49836610e-01  2.97321663e-03  6.34035363e-01\n",
      " -1.95932358e-01  1.31067537e-01 -4.49524459e-01  3.39191062e-01\n",
      "  7.03390182e-01  2.37241573e-01 -9.99138241e-02 -2.05409762e+00\n",
      " -1.63497024e+00  2.28198942e-01  6.35851166e-01  2.21090957e-02\n",
      "  4.57810710e-01  3.70551090e-02  2.41865531e-01 -8.17267715e-01\n",
      "  4.79416178e-01  2.51055154e-01 -1.38394467e+00  1.12116580e+00\n",
      " -1.14843890e-01  5.66383637e-01  2.55981232e-01  6.98264437e-01\n",
      " -1.79180623e+00 -1.55852933e+00 -1.11975661e+00  4.00079161e-01\n",
      "  9.20565367e-02  6.51644163e-01  4.34985050e-01 -7.01067599e-01\n",
      "  1.37622538e-01  7.95233080e-02 -3.58198164e-01  5.34661772e-01\n",
      "  3.80836214e-01  6.33968912e-02  1.12950237e-01 -5.16907852e-01\n",
      "  5.00908169e-01 -5.88531511e-01  7.40963543e-01  3.97816851e-01\n",
      " -8.22573315e-01  7.83102287e-01  1.41090022e-01  7.90638585e-01\n",
      " -1.20494064e+00 -1.06795684e+00 -5.12765639e-01 -1.31875819e-01\n",
      "  7.73152080e-02  6.03130085e-01  7.96752468e-02  5.38368760e-01\n",
      " -3.92941218e-01 -5.91217105e-01  8.36685974e-01  3.58967750e-01\n",
      "  7.28735886e-01  9.10691155e-01  3.11526862e-01  1.88229177e-01\n",
      "  9.55676948e-02  1.90778395e-01 -7.81888382e-01  3.94067568e-01\n",
      " -4.61758661e-01  4.74154037e-01  7.22515268e-01  2.57156347e-01\n",
      " -2.89479843e-01  7.58675479e-01  5.68318353e-01 -1.07117331e+00\n",
      "  6.48773225e-01  7.49176386e-02  2.64038515e-01  1.66526095e-01\n",
      "  1.13487944e-01 -2.82535543e+00 -1.73169529e+00  2.99791494e-01\n",
      "  2.59637815e-02  2.28612006e-01 -2.50870162e-01 -8.49697324e-01\n",
      " -1.31053924e+00 -7.88211388e-01 -4.31645663e-01 -5.62216707e-01\n",
      " -7.43469887e-01 -4.40930890e-01  4.21069847e-01  8.75283961e-02\n",
      " -8.21157377e-01  3.01846132e-01 -4.55497252e-01 -3.17498669e-01\n",
      "  3.95012362e-01  5.40192441e-01  8.61388984e-01 -4.86203180e-01\n",
      " -3.97192583e-02 -1.00655602e+00  2.02400057e-01 -7.25113237e-01\n",
      "  5.48441079e-01  4.66413603e-01  1.46354563e+00  7.91462612e-01\n",
      "  5.95748096e-01  5.95001656e-01  3.99481628e-02  1.30227907e-01\n",
      "  3.57140955e-01 -9.43253550e-02 -5.69837074e-01 -2.64758251e-01\n",
      " -1.21734257e+00 -1.38003584e+00 -5.28530906e-01 -3.07176665e-01\n",
      "  7.78497206e-01 -2.85955340e-01 -9.95912629e-01  6.16501145e-01\n",
      "  6.20503068e-01  1.05864305e+00 -3.75209599e-01  2.15739521e-01\n",
      " -2.12083969e+00  2.99583810e-01 -6.99759692e-02 -5.77286174e-02\n",
      " -2.41620702e-01  3.61827846e-01  8.85626114e-01  4.13518640e-01\n",
      " -1.33063711e-01  7.00001289e-01 -2.28331447e-01  3.41741684e-01\n",
      "  6.81994021e-01  4.53439200e-01 -1.11791709e+00 -1.22944897e-01\n",
      "  4.70969277e-01 -1.09613496e+00 -2.25662228e+00  7.06847332e-02\n",
      "  1.63429831e-01 -1.87210210e-01 -1.42983338e+00 -1.26789094e-03\n",
      " -1.42072623e-01  8.04036564e-01  3.85327633e-01 -7.67134193e-01\n",
      "  7.47040210e-01 -2.47293995e-01  5.68783468e-01  1.02792380e+00\n",
      " -1.41633004e+00 -4.92695787e-01 -1.51747425e+00  6.72482925e-01\n",
      "  8.85343880e-01 -8.39092838e-02  5.44437666e-02  6.06426369e-01\n",
      "  1.34387480e-01  2.08488418e-01  1.91501633e-01 -3.41009121e-01\n",
      " -4.48197741e-02  1.01720666e+00  6.44124239e-01  1.77124390e-02\n",
      "  3.94090205e-01  9.25370090e-02  6.36716626e-01 -3.50627839e-01\n",
      " -9.63372363e-03 -5.27247036e-01  4.13641040e-01  2.74058447e-01\n",
      "  8.23819283e-01  8.58715759e-02 -1.94167117e+00 -3.05919507e-01\n",
      "  1.68125354e-01  7.32170925e-02  7.20708141e-02  3.51142112e-02\n",
      "  2.89915476e-01  4.32392494e-01  6.13761512e-01 -7.87249737e-01\n",
      " -5.13054475e-01  4.24348597e-02 -2.47618882e-01  5.02199814e-01\n",
      " -3.71022213e-01  8.11706648e-01 -2.83137839e-01  3.05765389e-01\n",
      " -1.66134546e+00  5.57408806e-01  2.81727565e-01 -3.68323343e-01\n",
      " -6.74243529e-01  4.88114038e-01  9.04111146e-01 -9.87465355e-01\n",
      "  1.24453842e-01  8.49831537e-01 -6.28256494e-01 -5.61269878e-01\n",
      "  2.15360651e-01  9.15276640e-01 -1.53108475e+00 -7.58855748e-01\n",
      "  3.34979476e-01  3.59339579e-01 -2.16219089e-02  8.39002608e-01\n",
      "  3.03505599e-01  1.48872865e-01 -8.29108380e-01  9.10801812e-01\n",
      "  5.57134224e-01 -5.44499939e-01  4.02592504e-01 -9.24270128e-01\n",
      "  7.33831288e-01 -2.93259007e-01 -1.16413963e+00 -6.19182229e-01\n",
      "  5.79194177e-01  5.25219478e-01 -6.54873370e-01  2.09278185e-01\n",
      " -4.26893691e-01 -1.05397790e+00 -6.36021079e-01  2.06195325e-01\n",
      " -2.61609126e-01 -2.58807889e+00 -2.01876552e-01 -8.28876013e-01\n",
      "  2.93409153e-01 -1.76223469e-01  7.99411735e-02  9.47367873e-01\n",
      "  8.03378130e-01 -7.51236290e-01 -3.50160312e-01  1.35613801e-01\n",
      "  1.06159159e+00  2.54843384e-01  4.81329123e-01  6.56345594e-01\n",
      " -6.35187768e-01  5.23534637e-02 -8.32169380e-01 -1.67906069e+00\n",
      " -3.86859675e-01  2.63585920e-01 -8.26170616e-01 -4.01631140e-01\n",
      " -6.94517716e-02 -2.69866724e-02 -2.57442125e-01  2.27649041e-01\n",
      "  6.30383767e-01 -1.18435515e+00 -3.52125650e-01 -6.09082174e-01\n",
      " -5.51153782e-01  1.79755282e-01  8.10115763e-01  3.52164551e-01\n",
      " -4.48357747e-01 -4.53118776e-01  7.24073661e-01 -2.94325357e+00\n",
      " -2.24325257e+00  7.16415468e-01  5.91428585e-01  1.32633359e+00\n",
      "  5.34423130e-02  3.46442214e-01 -7.12105931e-01  3.64212266e-01\n",
      "  2.58175188e-02 -1.42277467e-01  2.83385294e-01 -2.16152595e-01\n",
      "  3.27615089e-01 -1.43672647e+00  2.00985066e-01  2.91920293e-01\n",
      " -1.81464805e+00 -1.19742893e+00 -2.48885970e-01 -1.34980425e+00\n",
      "  5.08391847e-01 -2.75687332e-01  5.64802719e-01 -4.85997913e-01\n",
      " -1.19777728e+00 -7.91423970e-01 -7.73093945e-01  7.11733942e-01\n",
      " -1.38274729e+00  4.13634799e-01 -1.83515055e+00 -1.14370208e-02\n",
      " -1.09828110e+00  1.55997176e-02 -3.44297537e-01  6.82610991e-01\n",
      " -1.29887169e+00  7.75102142e-01  1.30979348e-01 -1.55649978e+00\n",
      " -4.38424835e-01  6.73601525e-01  2.42927550e-01  4.82794351e-01\n",
      " -5.43100159e-01 -5.91403358e-01 -3.17721164e-01  2.40652140e-01\n",
      "  1.09168643e-01  4.26964721e-01 -4.52496777e-01 -3.05282721e-01\n",
      " -1.02389461e-01 -9.73445351e-01 -9.92488819e-01 -3.86993631e-01\n",
      "  5.48767902e-01  5.57218273e-01  9.89366521e-01 -7.27480156e-01\n",
      " -7.99308177e-01 -2.20814137e-01 -1.04119731e+00 -3.95383660e-01\n",
      "  8.62754840e-02 -1.08827914e-01  5.25908304e-01 -2.03932765e-02\n",
      "  3.40693987e-01 -1.32167497e+00  3.12170462e-01 -5.04223509e-01\n",
      " -1.05370441e-01 -6.43028970e-01  8.86532980e-01  4.05736294e-01\n",
      "  3.58618114e-02 -2.36423439e-02  6.82458920e-01  4.04125811e-01\n",
      " -3.35471206e-01  2.38030340e-01 -3.54767426e-01  4.21450046e-01\n",
      " -1.01830931e+00 -1.53728769e+00  3.86924068e-01 -1.69849862e+00\n",
      "  2.82455697e-01  5.24826319e-01  5.96107151e-01  4.80767051e-01\n",
      "  5.51717943e-01  1.72742191e-02  4.00441619e-01 -1.49230115e-01\n",
      "  4.15914577e-01  3.19263390e-01  2.46594869e-01 -1.85708906e-01\n",
      "  8.53002455e-01 -9.49523310e-02 -1.12796793e+00  9.43732719e-01\n",
      "  5.04276238e-01 -9.38478974e-01 -7.15880946e-01 -2.81541223e-01\n",
      " -1.35726083e+00  3.16535564e-01 -2.91858358e-01 -1.26735493e-01\n",
      " -4.72756146e-01 -1.11597033e+00 -2.11193998e-01  3.60723398e-01\n",
      " -1.11366027e+00  6.29818304e-01 -1.37833089e-01  6.49437435e-02\n",
      "  8.19035228e-01  4.52161961e-01  9.09600912e-01 -6.12854395e-01\n",
      "  8.06864529e-01 -3.89467398e-01  6.91841977e-02  2.14758934e-01\n",
      "  6.21979448e-01 -1.13849609e+00  4.65813856e-01 -1.55267275e+00\n",
      "  3.08379071e-01 -1.72076713e-01  7.13884434e-01  4.93069552e-01\n",
      " -1.39108940e-01 -4.97692482e-01 -7.80555025e-01  2.51547880e-01\n",
      " -1.87578986e-01  1.29876292e-01 -1.82207196e-01  3.62877435e-02\n",
      "  3.25592457e-01 -4.60880536e-01 -6.80690905e-01  1.25841528e-01\n",
      "  2.59834358e-01 -2.55204794e+00  1.62700185e-01 -1.86102780e-01\n",
      "  6.53251307e-01  7.94112217e-01  3.20523639e-01  4.45929289e-01\n",
      "  1.50332009e-01  9.04038364e-01 -4.16494425e-01 -1.71452431e+00]\n"
     ]
    }
   ],
   "source": [
    "print(Y_pred_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "print(Y_pred_10.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_10_actual = []\n",
    "for i in range(len(Y_pred_10)):\n",
    "    if Y_pred_10[i]>0:\n",
    "        Y_pred_10_actual.append(1)\n",
    "    else:\n",
    "        Y_pred_10_actual.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, -1, -1, -1, 1, -1, 1, 1, 1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, -1, 1, -1, -1, 1, -1, -1, -1, 1, -1, 1, -1, 1, -1, -1, -1, -1, -1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, 1, 1, -1, -1, -1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, -1, 1, -1, -1, 1, 1, 1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 1, 1, 1, -1, 1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, -1, 1, 1, 1, -1, -1, 1, -1, -1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, 1, -1, -1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1, 1, 1, 1, -1, 1, 1, -1, 1, -1, 1, -1, -1, -1, 1, 1, -1, 1, -1, -1, -1, 1, -1, -1, -1, -1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 1, 1, -1, -1, -1, -1, 1, 1, 1, -1, -1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, 1, -1, 1, 1, -1, -1, -1, -1, 1, -1, 1, -1, -1, -1, -1, 1, -1, 1, -1, -1, -1, 1, -1, 1, -1, 1, 1, -1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1, -1, -1, -1, -1, -1, -1, 1, 1, 1, -1, -1, -1, -1, -1, 1, -1, 1, -1, 1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, -1, 1, -1, 1, -1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 1, -1, -1, 1, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 1, -1, 1, -1, 1, 1, 1, 1, -1, 1, -1, 1, 1, 1, -1, 1, -1, 1, -1, 1, 1, -1, -1, -1, 1, -1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "print(Y_pred_10_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = test[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final testing accuracy for train sample size 10: 0.7240\n"
     ]
    }
   ],
   "source": [
    "final_acc_10 = (Y_pred_10_actual == Y_test).mean()\n",
    "print(\"final testing accuracy for train sample size 10: {:.4f}\".format(final_acc_10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for training set size 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "index50 = np.random.randint(0,2000,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_50 = X_train[index50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_50 = Y_train[index50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_50 = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor_50.fit(X_train_50, Y_train_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_50 = regressor_50.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_50_actual = []\n",
    "for i in range(len(Y_pred_50)):\n",
    "    if Y_pred_50[i]>0:\n",
    "        Y_pred_50_actual.append(1)\n",
    "    else:\n",
    "        Y_pred_50_actual.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 1, 1, 1, 1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 1, -1, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 1, 1, -1, -1, 1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 1, -1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, -1, 1, -1, -1, 1, 1, -1, -1, -1, -1, 1, -1, 1, 1, -1, 1, 1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 1, -1, 1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 1, -1, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 1, -1, 1, -1, -1, -1, 1, 1, -1, -1, -1, -1, -1, 1, 1, -1, -1, 1, 1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 1, -1, 1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1, 1, 1, -1, -1, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 1, 1, -1, -1, -1, 1, -1, 1, 1, -1, -1, -1, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, 1, -1, 1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "print(Y_pred_50_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final testing accuracy for train sample size 50: 0.8870\n"
     ]
    }
   ],
   "source": [
    "final_acc_50 = (Y_pred_50_actual == Y_test).mean()\n",
    "print(\"final testing accuracy for train sample size 50: {:.4f}\".format(final_acc_50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for training set size 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final testing accuracy for train sample size 100: 0.8910\n"
     ]
    }
   ],
   "source": [
    "index100 = np.random.randint(0,2000,100)\n",
    "X_train_100 = X_train[index100]\n",
    "Y_train_100 = Y_train[index100]\n",
    "regressor_100 = LinearRegression()\n",
    "regressor_100.fit(X_train_100, Y_train_100)\n",
    "Y_pred_100 = regressor_100.predict(X_test)\n",
    "Y_pred_100_actual = []\n",
    "for i in range(len(Y_pred_100)):\n",
    "    if Y_pred_100[i]>0:\n",
    "        Y_pred_100_actual.append(1)\n",
    "    else:\n",
    "        Y_pred_100_actual.append(-1)\n",
    "        \n",
    "final_acc_100 = (Y_pred_100_actual == Y_test).mean()\n",
    "print(\"final testing accuracy for train sample size 100: {:.4f}\".format(final_acc_100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for training size 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final testing accuracy for train sample size 500: 0.8970\n"
     ]
    }
   ],
   "source": [
    "index500 = np.random.randint(0,2000,500)\n",
    "X_train_500 = X_train[index500]\n",
    "Y_train_500 = Y_train[index500]\n",
    "regressor_500 = LinearRegression()\n",
    "regressor_500.fit(X_train_500, Y_train_500)\n",
    "Y_pred_500 = regressor_500.predict(X_test)\n",
    "Y_pred_500_actual = []\n",
    "for i in range(len(Y_pred_500)):\n",
    "    if Y_pred_500[i]>0:\n",
    "        Y_pred_500_actual.append(1)\n",
    "    else:\n",
    "        Y_pred_500_actual.append(-1)\n",
    "        \n",
    "final_acc_500 = (Y_pred_500_actual == Y_test).mean()\n",
    "print(\"final testing accuracy for train sample size 500: {:.4f}\".format(final_acc_500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for training set size 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr_10 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegr_10.fit(X_train_10, Y_train_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_lr_10 = logisticRegr_10.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      " -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1. -1.  1. -1. -1.  1. -1.  1.  1.  1.  1. -1.  1. -1. -1.\n",
      "  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1. -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1. -1. -1.\n",
      "  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1. -1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1. -1.  1.\n",
      " -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.\n",
      "  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1.  1.  1.\n",
      "  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.\n",
      "  1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1. -1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1. -1. -1.  1.  1.\n",
      "  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1. -1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1. -1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1. -1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1. -1. -1.  1.\n",
      " -1. -1. -1. -1.  1. -1.  1. -1. -1. -1. -1.  1. -1. -1.  1. -1.  1. -1.\n",
      " -1.  1. -1. -1. -1. -1.  1. -1.  1. -1. -1.  1. -1. -1. -1. -1. -1. -1.\n",
      " -1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
      " -1. -1. -1. -1.  1. -1.  1. -1. -1. -1.  1. -1. -1.  1. -1.  1. -1.  1.\n",
      " -1. -1. -1. -1. -1.  1.  1. -1. -1. -1. -1.  1.  1. -1. -1. -1.  1. -1.\n",
      "  1.  1. -1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1. -1.  1. -1.\n",
      "  1.  1. -1. -1. -1. -1. -1. -1. -1.  1.  1. -1. -1.  1.  1. -1.  1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1.\n",
      " -1. -1. -1. -1. -1.  1.  1. -1. -1. -1. -1. -1.  1.  1.  1.  1.  1.  1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.  1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1.  1.  1. -1.  1. -1. -1.  1.  1. -1. -1.  1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1.  1.  1. -1.  1. -1.  1.  1. -1. -1. -1.  1.\n",
      "  1. -1. -1.  1. -1. -1. -1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1. -1.\n",
      "  1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1.  1.  1. -1. -1. -1. -1.  1.\n",
      " -1.  1. -1. -1. -1.  1. -1. -1. -1.  1.  1. -1. -1.  1. -1. -1. -1.  1.\n",
      " -1. -1. -1.  1. -1.  1. -1. -1. -1.  1.  1. -1. -1. -1.  1. -1. -1. -1.\n",
      "  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1.  1. -1.\n",
      " -1. -1.  1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  1. -1. -1. -1. -1. -1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1.  1. -1. -1. -1.  1.\n",
      " -1.  1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1.  1.  1.  1. -1.\n",
      " -1. -1. -1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1. -1.  1.  1. -1. -1.\n",
      "  1.  1. -1. -1. -1.  1. -1. -1. -1. -1. -1.  1.  1.  1.  1. -1. -1. -1.\n",
      "  1. -1. -1. -1.  1. -1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1.  1. -1. -1.  1.  1.  1. -1.  1. -1. -1. -1.  1. -1.  1. -1.\n",
      " -1. -1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1.  1.  1. -1.  1. -1.  1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "print(Y_pred_lr_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "print(Y_pred_lr_10.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final testing accuracy for train sample size 10: 0.8080\n"
     ]
    }
   ],
   "source": [
    "final_acc_lr_10 = (Y_pred_lr_10 == Y_test).mean()\n",
    "print(\"final testing accuracy for train sample size 10: {:.4f}\".format(final_acc_lr_10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for training set size 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final testing accuracy for train sample size 50: 0.9160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logisticRegr_50 = LogisticRegression()\n",
    "logisticRegr_50.fit(X_train_50, Y_train_50)\n",
    "Y_pred_lr_50 = logisticRegr_50.predict(X_test)\n",
    "final_acc_lr_50 = (Y_pred_lr_50 == Y_test).mean()\n",
    "print(\"final testing accuracy for train sample size 50: {:.4f}\".format(final_acc_lr_50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for training set size 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final testing accuracy for train sample size 100: 0.9340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logisticRegr_100 = LogisticRegression()\n",
    "logisticRegr_100.fit(X_train_100, Y_train_100)\n",
    "Y_pred_lr_100 = logisticRegr_100.predict(X_test)\n",
    "final_acc_lr_100 = (Y_pred_lr_100 == Y_test).mean()\n",
    "print(\"final testing accuracy for train sample size 100: {:.4f}\".format(final_acc_lr_100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for training set size 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final testing accuracy for train sample size 500: 0.9350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logisticRegr_500 = LogisticRegression()\n",
    "logisticRegr_500.fit(X_train_500, Y_train_500)\n",
    "Y_pred_lr_500 = logisticRegr_500.predict(X_test)\n",
    "final_acc_lr_500 = (Y_pred_lr_500 == Y_test).mean()\n",
    "print(\"final testing accuracy for train sample size 500: {:.4f}\".format(final_acc_lr_500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
